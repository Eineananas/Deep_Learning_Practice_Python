{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e34d423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ELMO: https://zhuanlan.zhihu.com/p/51679783\n",
    "# 层次表示：https://www.zhihu.com/question/475118382\n",
    "# 代码：https://zhuanlan.zhihu.com/p/53803919\n",
    "# 代码：https://blog.csdn.net/CSTGYinZong/article/details/121833353\n",
    "# 中文ELMO: https://blog.csdn.net/ccbrid/article/details/90545836\n",
    "# 记录elmoformanylangs安装：https://blog.csdn.net/qq_39915004/article/details/108545253\n",
    "# 工具包版本问题：https://github.com/HIT-SCIR/ELMoForManyLangs/issues/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfde4b4f",
   "metadata": {},
   "source": [
    "### 英文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa23f147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce7f9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "options_file = \"options.json\" # 配置文件地址 \n",
    "weight_file = \"weights.hdf5\" # 权重文件地址\n",
    "\n",
    "# 这里的1表示产生一组线性加权的词向量。\n",
    "# 如果改成2 即产生两组不同的线性加权的词向量。\n",
    "elmo = Elmo(options_file, weight_file, 1, dropout=0)\n",
    "\n",
    "# use batch_to_ids to convert sentences to character ids\n",
    "sentence_lists = [['I', 'love', 'you', '.'], ['Sorry', ',', 'I', 'don', \"'t\", 'love', 'you', '.']]\n",
    "character_ids = batch_to_ids(sentence_lists)\n",
    "\n",
    "embeddings = elmo(character_ids)['elmo_representations'][0]\n",
    "#elmo_mask = elmo(character_ids)['mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff180743",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe6cdc23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8, 50])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6886cd71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[259,  74, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 109, 112, 119, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 122, 112, 118, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0],\n",
       "         [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "            0,   0,   0,   0,   0,   0,   0,   0]],\n",
       "\n",
       "        [[259,  84, 112, 115, 115, 122, 260, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  45, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  74, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 101, 112, 111, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  40, 117, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 109, 112, 119, 102, 260, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259, 122, 112, 118, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261],\n",
       "         [259,  47, 260, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261, 261,\n",
       "          261, 261, 261, 261, 261, 261, 261, 261]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a6af7b",
   "metadata": {},
   "source": [
    "### 中文"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cdb394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elmoformanylangs import Embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1a405e22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-21 18:28:10,578 WARNING: Could not find config.  Trying D:/jupyter_file/textana/ELMO/zhs.model/cnn_50_100_512_4096_sample.json\n",
      "2023-02-21 18:28:10,579 WARNING: Could not find config.  Trying D:\\software\\Anaconda\\lib\\site-packages\\elmoformanylangs-0.0.4.post2-py3.8.egg\\elmoformanylangs\\configs\\cnn_50_100_512_4096_sample.json\n",
      "2023-02-21 18:28:10,621 INFO: char embedding size: 6169\n",
      "2023-02-21 18:28:11,029 INFO: word embedding size: 71222\n",
      "2023-02-21 18:28:16,956 INFO: Model(\n",
      "  (token_embedder): ConvTokenEmbedder(\n",
      "    (word_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(71222, 100, padding_idx=3)\n",
      "    )\n",
      "    (char_emb_layer): EmbeddingLayer(\n",
      "      (embedding): Embedding(6169, 50, padding_idx=6166)\n",
      "    )\n",
      "    (convolutions): ModuleList(\n",
      "      (0): Conv1d(50, 32, kernel_size=(1,), stride=(1,))\n",
      "      (1): Conv1d(50, 32, kernel_size=(2,), stride=(1,))\n",
      "      (2): Conv1d(50, 64, kernel_size=(3,), stride=(1,))\n",
      "      (3): Conv1d(50, 128, kernel_size=(4,), stride=(1,))\n",
      "      (4): Conv1d(50, 256, kernel_size=(5,), stride=(1,))\n",
      "      (5): Conv1d(50, 512, kernel_size=(6,), stride=(1,))\n",
      "      (6): Conv1d(50, 1024, kernel_size=(7,), stride=(1,))\n",
      "    )\n",
      "    (highways): Highway(\n",
      "      (_layers): ModuleList(\n",
      "        (0): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "        (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
      "      )\n",
      "    )\n",
      "    (projection): Linear(in_features=2148, out_features=512, bias=True)\n",
      "  )\n",
      "  (encoder): ElmobiLm(\n",
      "    (forward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_0): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (forward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "    (backward_layer_1): LstmCellWithProjection(\n",
      "      (input_linearity): Linear(in_features=512, out_features=16384, bias=False)\n",
      "      (state_linearity): Linear(in_features=512, out_features=16384, bias=True)\n",
      "      (state_projection): Linear(in_features=4096, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "2023-02-21 18:28:17,406 INFO: 1 batches, avg len: 9.5\n"
     ]
    }
   ],
   "source": [
    "e = Embedder('D:/jupyter_file/textana/ELMO/zhs.model/')  #绝对路径\n",
    "\n",
    "sents = [['今', '天', '天氣', '真', '好', '阿'],\n",
    "['潮水', '退', '了', '就', '知道', '誰', '沒', '穿', '褲子']]\n",
    "# the list of lists which store the sentences \n",
    "# after segment if necessary.\n",
    "\n",
    "ch_em = e.sents2elmo(sents,output_layer=-1)\n",
    "# will return a list of numpy arrays \n",
    "# each with the shape=(seq_len, embedding_size)\n",
    "\n",
    "# output_layer: the target layer to output.\n",
    "# 0 for the word encoder\n",
    "# 1 for the first LSTM hidden layer\n",
    "# 2 for the second LSTM hidden layer\n",
    "# -1 for an average of 3 layers. (default)\n",
    "# -2 for all 3 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f644637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1024)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_em[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f7a0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
